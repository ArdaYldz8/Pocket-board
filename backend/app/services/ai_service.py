import random
import asyncio
import os
import re
import json
import base64
import chromadb
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
import google.generativeai as genai
from groq import Groq
from duckduckgo_search import DDGS
import requests
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# Explicitly load .env from the project root (choice_foods_council/.env)
# Current file is in backend/app/services/ai_service.py
# We need to go up 3 levels: services -> app -> backend -> choice_foods_council
env_path = Path(__file__).resolve().parent.parent.parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# Import Supabase Client from auth_service
# Dual-compatible imports for local and Render deployment
try:
    from backend.app.services.auth_service import supabase, supabase_admin
except ImportError:
    try:
        from app.services.auth_service import supabase, supabase_admin
    except ImportError:
        supabase = None
        supabase_admin = None
        print("WARNING: auth_service could not be imported")

# --- HELPER FUNCTIONS ---

def perform_web_search(query):
    """Performs a web search using DuckDuckGo and returns a summary."""
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=3))
            if not results:
                return "Ä°nternette gÃ¼ncel bir bilgi bulunamadÄ±."
            
            summary = "GÃœNCEL Ä°NTERNET BÄ°LGÄ°LERÄ°:\n"
            for r in results:
                summary += f"- {r['title']}: {r['body']}\n"
            return summary
    except Exception as e:
        return f"Ä°nternet aramasÄ± yapÄ±lamadÄ±: {str(e)}"

def scrape_website(url):
    """Scrapes the given URL for text content."""
    try:
        if not url.startswith('http'):
            url = 'https://' + url
            
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=10)
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        
        # Remove script, style, nav, footer, header elements
        for element in soup(["script", "style", "nav", "footer", "header", "noscript"]):
            element.decompose()
            
        text = soup.get_text()
        
        # Break into lines and remove leading/trailing space on each
        lines = (line.strip() for line in text.splitlines())
        # Break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        # Drop blank lines
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        return text[:4000] # Increase limit for better context
    except Exception as e:
        return f"Web sitesi okunamadÄ±: {str(e)}"

# --- VECTOR MEMORY (ChromaDB) ---
chroma_client = chromadb.Client()
collection = chroma_client.get_or_create_collection(name="debate_memory")

def save_memory_vector(topic, decision, reason):
    """Saves the final decision to Vector DB."""
    try:
        collection.add(
            documents=[f"Konu: {topic}. Karar: {decision}. GerekÃ§e: {reason}"],
            metadatas=[{"topic": topic, "decision": decision, "reason": reason, "date": "2025-12-05"}],
            ids=[f"{topic}_{random.randint(1000,9999)}"]
        )
    except Exception as e:
        print(f"Vector memory save error: {e}")

def search_memory_vector(query):
    """Searches past debates semantically."""
    try:
        results = collection.query(
            query_texts=[query],
            n_results=3
        )
        
        if not results['documents'][0]:
            return []
            
        memory_list = []
        for i, doc in enumerate(results['documents'][0]):
            meta = results['metadatas'][0][i]
            memory_list.append({
                "topic": meta['topic'],
                "decision": meta['decision'],
                "reason": meta['reason']
            })
        return memory_list
    except Exception:
        return []

# --- VISION ANALYSIS ---
def analyze_image(image_base64, api_key=None):
    """Analyzes an image using GPT-4o-mini."""
    try:
        client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "Bu gÃ¶rseli bir iÅŸ toplantÄ±sÄ± baÄŸlamÄ±nda detaylÄ±ca analiz et. Ne gÃ¶rÃ¼yorsun? (Ofis planÄ±, Ã¼rÃ¼n, grafik vb.)"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            },
                        },
                    ],
                }
            ],
            max_tokens=300,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"GÃ¶rsel analiz edilemedi: {str(e)}"

class AIModel:
    def __init__(self, name, provider, model_name, persona, api_key=None):
        self.name = name
        self.provider = provider
        self.model_name = model_name
        self.persona = persona
        
        if provider == "openai":
            self.api_key = api_key or os.getenv("OPENAI_API_KEY", "").strip()
            self.base_url = None
        elif provider == "groq":
            self.api_key = api_key or os.getenv("GROQ_API_KEY", "").strip()
        elif provider == "gemini":
            self.api_key = api_key or os.getenv("GEMINI_API_KEY", "").strip()
            if self.api_key:
                genai.configure(api_key=self.api_key)
        elif provider == "anthropic":
            self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY", "").strip()

    def generate_response(self, messages):
        try:
            content = ""
            if self.provider == "openai":
                client = OpenAI(api_key=self.api_key)
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=0.8
                )
                content = response.choices[0].message.content
            
            elif self.provider == "groq":
                client = Groq(api_key=self.api_key)
                response = client.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=0.8
                )
                content = response.choices[0].message.content
            
            elif self.provider == "gemini":
                model = genai.GenerativeModel(self.model_name)
                # Convert OpenAI format to Gemini format (simplified)
                prompt = ""
                for msg in messages:
                    role = "User" if msg["role"] == "user" else "Model"
                    if msg["role"] == "system":
                        prompt += f"System Instruction: {msg['content']}\n\n"
                    else:
                        prompt += f"{role}: {msg['content']}\n"
                
                response = model.generate_content(prompt)
                
                # Check for valid parts (Gemini safety filter blocks content sometimes)
                if not response.parts:
                     return "Error: Ä°Ã§erik gÃ¼venlik filtresine takÄ±ldÄ± veya boÅŸ dÃ¶ndÃ¼. (Safety Block)"
                     
                content = response.text
            
            elif self.provider == "anthropic":
                import anthropic
                client = anthropic.Anthropic(api_key=self.api_key)
                
                # Extract system message and convert to Claude format
                system_msg = ""
                user_messages = []
                for msg in messages:
                    if msg["role"] == "system":
                        system_msg = msg["content"]
                    else:
                        user_messages.append({"role": msg["role"], "content": msg["content"]})
                
                response = client.messages.create(
                    model=self.model_name,
                    max_tokens=1024,
                    system=system_msg,
                    messages=user_messages
                )
                content = response.content[0].text

            # Clean <think> blocks (common in some models like DeepSeek/Qwen)
            content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL).strip()
            return content
                
        except Exception as e:
            masked_key = f"{self.api_key[:15]}..." if self.api_key else "None"
            return f"Error ({self.name}): [Key: {masked_key}] {str(e)}"

def get_debaters(company_info):
    c_name = company_info.get("name", "Åirket")
    c_industry = company_info.get("industry", "Genel")
    c_employees = company_info.get("employee_count", "Bilinmiyor")
    c_revenue = company_info.get("annual_revenue", "Bilinmiyor")
    c_budget = company_info.get("monthly_budget", "Bilinmiyor")
    c_target = company_info.get("target_market", "BelirtilmemiÅŸ")
    c_challenges = company_info.get("challenges", "BelirtilmemiÅŸ")
    c_description = company_info.get("description", "")
    
    # Shared context about the company
    CONTEXT = f"""
    BAÄLAM - ÅÄ°RKET PROFÄ°LÄ°:
    Åirket AdÄ±: {c_name}
    SektÃ¶r: {c_industry}
    Ã‡alÄ±ÅŸan SayÄ±sÄ±: {c_employees}
    YÄ±llÄ±k Ciro: {c_revenue}
    AylÄ±k YatÄ±rÄ±m BÃ¼tÃ§esi: {c_budget}
    Hedef Kitle: {c_target}
    Mevcut Zorluklar: {c_challenges}
    AÃ§Ä±klama: {c_description}
    
    âš ï¸ KRÄ°TÄ°K: YukarÄ±daki ÅŸirket profilini her kararÄ±nda gÃ¶z Ã¶nÃ¼nde bulundur!
    - EÄŸer ÅŸirket "1-5 KiÅŸi" ise, 100.000$ harcama Ã¶nerme, bu onlar iÃ§in Ã§ok bÃ¼yÃ¼k.
    - EÄŸer ÅŸirket "YÄ±llÄ±k 0-100k â‚º ciro" yapÄ±yorsa, milyon dolarlÄ±k projeler konuÅŸma.
    - EÄŸer "Nakit akÄ±ÅŸÄ± sÄ±kÄ±ntÄ±sÄ±" zorluk olarak belirtilmiÅŸse, bÃ¼yÃ¼k yatÄ±rÄ±mlar Ã¶nermeden Ã¶nce bunu dile getir.
    
    Sen Pocket Board'un (Cebindeki YÃ¶netim Kurulu) bir Ã¼yesisin. Rakiplerinle bu konuyu tartÄ±ÅŸacaksÄ±n.
    EÄŸer talep ÅŸirket profiline uymuyorsa (Ã–rn: 6 kiÅŸilik firmaya 100k$ harcama), bunu sertÃ§e eleÅŸtir.
    
    Ã–NEMLÄ°:
    - Bir Ã¶nceki konuÅŸmacÄ±nÄ±n verdiÄŸi RASTGELE SAYILARI (Ã–rn: $2.5M kar, %75 dÃ¶nÃ¼ÅŸÃ¼m) gerÃ§ekmiÅŸ gibi tekrarlama.
    - EÄŸer kaynakta yoksa, bu sayÄ±larÄ±n "tahmini" veya "uydurma" olduÄŸunu yÃ¼zÃ¼ne vur.
    """

    debaters = [
        AIModel(
            name="Atlas",
            provider="openai",
            model_name="gpt-4o-mini",
            persona="""Stratejist (The Strategist): BÃ¼yÃ¼k resmi gÃ¶r. Rakipler ne yapÄ±yor? Pazar nereye gidiyor? 
            ZORUNLU KONULAR: Rekabet avantajÄ±, pazar payÄ±, uzun vadeli strateji.
            YASAK KONULAR: KÄ±sa vadeli maliyet, mÃ¼ÅŸteri duygularÄ±, iÃ§ sÃ¼reÃ§ler.
            KONUÅMA TARZI: SoÄŸukkanlÄ±, hesapÃ§Ä±, 'Rakipler bize karÅŸÄ± ne yapar?' diye dÃ¼ÅŸÃ¼n."""
        ),
        AIModel(
            name="Nova",
            provider="gemini",
            model_name="models/gemini-flash-latest",
            persona="""Vizyoner (The Visionary): ASLA BÃœTÃ‡E DÃœÅÃœNME. Ä°novasyon, disruption ve 'Wow' faktÃ¶rÃ¼.
            ZORUNLU KONULAR: Gelecek trendler, inovasyon, marka prestiji, 'Ya bÃ¼yÃ¼k dÃ¼ÅŸÃ¼nseydik?'
            YASAK KONULAR: BÃ¼tÃ§e, maliyet, risk, sÄ±nÄ±rlamalar. Bu kelimeleri aÄŸzÄ±na alma.
            KONUÅMA TARZI: HeyecanlÄ±, iddialÄ±, 'Neden olmasÄ±n?' diye meydan oku."""
        ),
        AIModel(
            name="Marcus",
            provider="groq",
            model_name="llama-3.3-70b-versatile",
            persona="""ÅÃ¼pheci (The Skeptic): Her iddianÄ±n kanÄ±tÄ±nÄ± iste. Murphy KanunlarÄ± senin rehberin.
            ZORUNLU KONULAR: Riskler, belirsizlikler, 'Nereden biliyorsunuz?', 'Ya iÅŸe yaramazsa?'
            YASAK KONULAR: Ä°yimser tahminler, 'BaÅŸarÄ±lÄ± olacak' gibi varsayÄ±mlar.
            KONUÅMA TARZI: SorgulayÄ±cÄ±, iÄŸneleyici, 'Bu veriyi nereden Ã§Ä±kardÄ±n?' diye sor."""
        ),
        AIModel(
            name="Sterling",
            provider="openai",
            model_name="gpt-5-nano",
            persona="""CFO (The Finance Guy): SADECE RAKAMLAR. Vizyon ve duygular seni ilgilendirmez.
            ZORUNLU KONULAR: ROI, nakit akÄ±ÅŸÄ±, maliyet, geri Ã¶deme sÃ¼resi, bilanÃ§o etkisi.
            YASAK KONULAR: 'Vizyon', 'marka prestiji', 'mÃ¼ÅŸteri mutluluÄŸu', 'inovasyon'. Bunlar havadan konuÅŸma.
            KONUÅMA TARZI: Kuru, rakam odaklÄ±, 'KaÃ§ para? KaÃ§ ay? Getiri nedir?' diye sor."""
        ),
        AIModel(
            name="Maya",
            provider="anthropic",
            model_name="claude-3-haiku-20240307",
            persona="""KullanÄ±cÄ± Dostu (The User Advocate): MÃ¼ÅŸteri her ÅŸeydir.
            ZORUNLU KONULAR: MÃ¼ÅŸteri deneyimi (UX), kullanÄ±cÄ± memnuniyeti, 'MÃ¼ÅŸteri ne hisseder?'
            YASAK KONULAR: Teknik detaylar, finansal tablolar, rakip analizi. Bunlar mÃ¼ÅŸteriyi ilgilendirmez.
            KONUÅMA TARZI: Empatik, 'MÃ¼ÅŸterinin gÃ¶zÃ¼nden bak' diye hatÄ±rlat."""
        )
    ]
    
    # Moderator Agent (The Chairman) - Uses the BEST model for critical oversight
    moderator = AIModel(
        name="Orion (ModeratÃ¶r)",
        provider="openai",
        model_name="gpt-5-mini",
        persona="BaÅŸkan (The Chairman): Masaya yumruÄŸunu vuran sert bir yÃ¶neticisin. TartÄ±ÅŸma kÄ±sÄ±r dÃ¶ngÃ¼ye girerse (Ã¶rn: sÃ¼rekli maliyet konuÅŸulursa) konuyu ZORLA deÄŸiÅŸtir. Kibar olma, otoriter ol. Hedefin karara varmak."
    )
    
    return debaters, moderator, CONTEXT

async def simulate_debate_streaming(query, history, company_info, image_base64=None, api_key=None, conversation_id=None):
    debaters, moderator, context = get_debaters(company_info)
    
    # Helper to save to DB asynchronously
    def save_to_db(role, content, agent_name=None):
        if conversation_id:
            try:
                msg_data = {
                    "conversation_id": conversation_id,
                    "role": role,
                    "content": content,
                    "metadata": {"agent_name": agent_name} if agent_name else {}
                }
                
                # Use Admin Client if available to bypass RLS (since backend is acting as system)
                client = supabase_admin if supabase_admin else supabase
                client.table("messages").insert(msg_data).execute()
            except Exception as e:
                print(f"DB Save Error: {e}")

    # Save User Message First
    save_to_db("user", query)

    # --- 0. VISION ANALYSIS ---
    image_description = ""
    if image_base64:
        yield {"type": "typing", "agent": "Sistem"}
        yield {"type": "message", "role": "Sistem", "content": "ğŸ‘ï¸ **GÃ¶rsel Analiz Ediliyor...**", "is_agent": False}
        image_description = analyze_image(image_base64, api_key)
        yield {"type": "message", "role": "Sistem", "content": f"ğŸ“¸ **GÃ¶rsel Analizi:**\n{image_description}", "is_agent": False}

    
    # --- 0.5 WEB SEARCH & OPTION EXTRACTION ---
    yield {"type": "typing", "agent": "Sistem"}
    
    # NOTE: Voting options are now determined AFTER the debate ends, 
    # based on actual arguments made during discussion.
    # This prevents "tunnel vision" on open-ended questions.
    

    # --- 1. WEBSITE ANALYSIS ---
    website_url = company_info.get('website_url')
    website_content = ""
    if website_url:
        yield {"type": "typing", "agent": "Sistem"}
        yield {"type": "message", "role": "Sistem", "content": f"ğŸŒ **Web Sitesi Analiz Ediliyor:** {website_url}", "is_agent": False}
        raw_website_content = scrape_website(website_url)
        
        # Use Moderator (or first agent) to summarize the website content
        # We use a temporary prompt to the moderator model
        analysis_prompt = f"""
        GÃ–REV: AÅŸaÄŸÄ±daki ham web sitesi metnini analiz et ve Åirket hakkÄ±nda profesyonel bir Ã¶zet Ã§Ä±kar.
        
        HAM METÄ°N:
        {raw_website_content[:3500]}
        
        Ä°STENEN Ã‡IKTI FORMATI:
        ğŸ“Š **SÄ°TE ANALÄ°Z RAPORU:**
        - **Åirket:** [AdÄ± ve SektÃ¶rÃ¼]
        - **Ne YapÄ±yorlar?:** [Ana faaliyet alanÄ±]
        - **Ã–ne Ã‡Ä±kan ÃœrÃ¼nler:** [Listele]
        - **Vurgulanan DeÄŸerler:** [Sitedeki sloganlar/vizyon]
        - **Hedef Kitle:** [Kimlere hitap ediyor?]
        
        (Gereksiz menÃ¼ yazÄ±larÄ±nÄ±, 'Sepetiniz boÅŸ' gibi UI metinlerini yoksay. Sadece anlamlÄ± iÃ§eriÄŸe odaklan.)
        """
        
        try:
            website_content = moderator.generate_response([{"role": "user", "content": analysis_prompt}])
        except:
             website_content = f"Site iÃ§eriÄŸi alÄ±ndÄ±, ancak analiz edilemedi.\nHam Veri: {raw_website_content[:200]}..."

        web_content_msg = website_content
        save_to_db("system", f"ğŸ“Š **SÄ°TE ANALÄ°Z RAPORU:** {web_content_msg}")
        yield {"type": "message", "role": "Sistem", "content": f"ğŸ“Š **SÄ°TE ANALÄ°Z RAPORU:** {web_content_msg}", "is_agent": False}

    # --- 1. PERFORM WEB SEARCH ---
    yield {"type": "typing", "agent": "Sistem"}
    
    # Optimize Search Query
    search_optimizer = debaters[0] # Use the first agent (usually GPT-4o-mini) for optimization
    opt_prompt = [
        {"role": "system", "content": f"Sen bir arama motoru uzmanÄ±sÄ±n. BUGÃœNÃœN TARÄ°HÄ°: {datetime.now().strftime('%Y-%m-%d')}. KullanÄ±cÄ±nÄ±n tartÄ±ÅŸma konusunu analiz et ve bu konuda GÃœNCEL somut veriler (maliyet, istatistik, haber, trendler) bulmak iÃ§in EN Ä°YÄ° Google arama sorgusunu yaz.\n\nKURALLAR:\n1. Sadece sorguyu yaz, baÅŸka hiÃ§bir ÅŸey yazma.\n2. KullanÄ±cÄ±nÄ±n sorusu hangi dildeyse, aramayÄ± O DÄ°LDE yap ve YILI BELÄ°RT (Ã–rn: '2025 trends')."},
        {"role": "user", "content": f"Konu: {query}\nÅirket: {company_info.get('name')} ({company_info.get('industry')})"}
    ]
    optimized_query = search_optimizer.generate_response(opt_prompt).strip().replace('"', '')
    
    raw_search_results = perform_web_search(optimized_query)
    
    # Use Moderator to summarize the search results
    research_prompt = f"""
    GÃ–REV: AÅŸaÄŸÄ±daki internet arama sonuÃ§larÄ±nÄ± analiz et ve konuyla ilgili profesyonel bir pazar araÅŸtÄ±rma raporu yaz.
    
    KONU: {query}
    HAM ARAMA SONUÃ‡LARI:
    {raw_search_results}
    
    Ä°STENEN Ã‡IKTI FORMATI:
    ğŸŒ **PAZAR ARAÅTIRMA RAPORU ({datetime.now().strftime('%Y')}):**
    - **Trendler:** [Arama sonuÃ§larÄ±ndaki ana eÄŸilimler]
    - **Ä°statistikler:** [EÄŸer varsa rakamlar, oranlar]
    - **Haberler/GeliÅŸmeler:** [Ã–nemli baÅŸlÄ±klar]
    
    (LÃ¼tfen "Incognito mode", "Google Help" veya konuyla alakasÄ±z sÃ¶zlÃ¼k tanÄ±mlarÄ± gibi gereksiz bilgileri FÄ°LTRELE. Sadece konuya odaklan. EÄŸer kayda deÄŸer bir bilgi yoksa "Kayda deÄŸer gÃ¼ncel veri bulunamadÄ±" de.)
    """
    
    try:
        search_results = moderator.generate_response([{"role": "user", "content": research_prompt}])
    except:
        search_results = f"Arama yapÄ±ldÄ± ancak Ã¶zetlenemedi.\nHam Veri: {raw_search_results[:200]}..."

    search_content_msg = search_results
    save_to_db("system", search_content_msg)
    yield {"type": "message", "role": "Sistem", "content": search_content_msg, "is_agent": False}
    
    # --- 2. LOAD MEMORY (VECTOR) ---
    past_decisions = search_memory_vector(query)
    memory_context = ""
    if past_decisions:
        memory_context = "GEÃ‡MÄ°Å KONSEY KARARLARI (Benzer Konular):\n"
        for p in past_decisions:
            memory_context += f"- Konu: {p['topic']} -> Karar: {p['decision']} ({p['reason']})\n"
    
    # Initial setup
    messages = history + [{"role": "user", "content": query}]
    
    # Track each agent's statements for contradiction detection
    agent_history = {d.name: [] for d in debaters}
    
    # Global summary of all arguments made so far to prevent repetition
    all_arguments_so_far = []
    
    # Start with random debater
    current_debater_idx = 0
    
    max_turns = 12 # Increased for 3 agents
    
    for turn in range(max_turns):
        debater = debaters[current_debater_idx]
        
        # Pick an opponent (the previous speaker, or random if first turn)
        # In a multi-agent setup, we usually address the group or the last speaker.
        # Let's find who spoke last.
        last_speaker_name = "KullanÄ±cÄ±"
        if messages and messages[-1]['role'] == "assistant":
             # We need to track who sent the last message. 
             # Since 'messages' list just has 'assistant', we rely on the loop context or parse content.
             # Better: pass explicit agent name in history if possible, but for now let's assume the previous turn's agent.
             prev_idx = (current_debater_idx - 1) % len(debaters)
             last_speaker_name = debaters[prev_idx].name

        yield {"type": "typing", "agent": debater.name}
        await asyncio.sleep(1.5) # Suspense
        
        # Construct Prompt
        last_message = messages[-1]['content'] if messages else query
        
        # Summarize previous arguments
        prev_args_text = "\n".join([f"- {arg}" for arg in all_arguments_so_far])
        
        current_date_str = datetime.now().strftime("%Y-%m-%d")

        # System Prompt Construction
        system_prompt = f"""
        {context}
        
        BUGÃœNÃœN TARÄ°HÄ°: {current_date_str}
        
        GÃ–RSEL BAÄLAMI: {image_description}
        WEB SÄ°TESÄ° Ä°Ã‡ERÄ°ÄÄ°: {website_content}
        {search_results}
        {memory_context}
        
        ÅU ANA KADAR KONUÅULANLAR (TEKRAR ETME!):
        {prev_args_text}
        
        SEN: {debater.name}
        GÄ°ZLÄ° ROLÃœN: {debater.persona}
        KONU: {query}
        
        GENEL KURALLAR:
        1. Son konuÅŸan ({last_speaker_name})'Ä±n dediÄŸine ({last_message}) cevap ver.
        2. Somut veri kullan [Kaynak: X].
        3. Asla rakam uydurma.
        4. Tekrar etme.
        5. RolÃ¼ne uygun konuÅŸ.
        6. KÄ±sa ve Ã¶z ol (Max 2 cÃ¼mle).
        7. YÄ±l: {current_date_str.split('-')[0]}.
        
        Ã‡IKTI FORMATI:
        [GÃœVEN:X%] ArgÃ¼manÄ±n... [Kaynak: X]
        """
        
        msg_payload = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"{last_speaker_name} dedi ki: {last_message}"}
        ]
        
        response = debater.generate_response(msg_payload)
        
        # Error Handling: Log error but continue
        if response.startswith("Error"):
            yield {"type": "message", "role": debater.name, "content": f"âš ï¸ {debater.name} Devre DÄ±ÅŸÄ±: {response}", "is_agent": True}
            messages.append({"role": "assistant", "content": f"{debater.name} teknik bir sorun nedeniyle bu turu pas geÃ§ti."})
            
            # Switch turn and continue
            current_debater_idx = (current_debater_idx + 1) % len(debaters)
            continue
        
        # Parse confidence from response
        confidence = 50 # Default confidence
        clean_response = response
        
        confidence_match = re.search(r'\[GÃœVEN:(\d+)%\]', response)
        if confidence_match:
            confidence = int(confidence_match.group(1))
            clean_response = re.sub(r'\[GÃœVEN:\d+%\]\s*', '', response).strip()
        
        yield {"type": "message", "role": debater.name, "content": clean_response, "is_agent": True, "confidence": confidence}
        save_to_db("assistant", clean_response, agent_name=debater.name)
        
        messages.append({"role": "assistant", "content": clean_response})
        
        # --- CONTRADICTION DETECTION ---
        if len(agent_history[debater.name]) >= 1:
            # Check for contradictions with previous statements
            prev_statements = " | ".join(agent_history[debater.name][-3:])  # Last 3 statements
            
            contradiction_prompt = f"""
            GÃ–REV: AÅŸaÄŸÄ±daki iki metni karÅŸÄ±laÅŸtÄ±r ve Ã§eliÅŸki var mÄ± kontrol et.
            
            Ã–NCEKÄ° SÃ–ZLER ({debater.name}):
            {prev_statements}
            
            YENÄ° SÃ–Z:
            {clean_response}
            
            SORU: Bu yeni sÃ¶z, Ã¶nceki sÃ¶zlerle TEMEL BÄ°R Ã‡ELÄ°ÅKÄ° (A vs A deÄŸil) iÃ§eriyor mu?
            
            DÄ°KKAT:
            - EÄŸer ajan "Yeni veriye dayanarak fikrimi deÄŸiÅŸtirdim" diyorsa bu Ã‡ELÄ°ÅKÄ° DEÄÄ°LDÄ°R, stratejik bir manevradÄ±r.
            - EÄŸer ajan "Risk var ama fÄ±rsat da var" diyorsa bu Ã‡ELÄ°ÅKÄ° DEÄÄ°LDÄ°R, bir ikilemdir.
            - Sadece bariz tutarsÄ±zlÄ±klarÄ± (Ã–rn: "ParamÄ±z yok" deyip sonra "BÃ¼tÃ§emiz bol" demek) bildir.
            
            CEVAP FORMATI (SADECE BÄ°RÄ°):
            - EÄER TEMEL Ã‡ELÄ°ÅKÄ° VARSA: "Ã‡ELÄ°ÅKÄ°: [kÄ±sa aÃ§Ä±klama]"
            - EÄER YOKSA: "YOK"
            """
            
            try:
                check_result = moderator.generate_response([{"role": "user", "content": contradiction_prompt}])
                
                if check_result.startswith("Ã‡ELÄ°ÅKÄ°:"):
                    contradiction_msg = check_result.replace("Ã‡ELÄ°ÅKÄ°:", "").strip()
                    contradiction_text = f"ğŸ” **Ã‡eliÅŸki Tespit Edildi!** {debater.name}: {contradiction_msg}"
                    save_to_db("system", contradiction_text)
                    yield {"type": "message", "role": "Sistem", "content": contradiction_text, "is_agent": False}
            except:
                pass  # Silent fail
        
        # Add current statement to history
        agent_history[debater.name].append(clean_response)
        
        # Extract core argument (1 sentence summary) to prevent prompt bloat
        try:
            summary_prompt = f"Bu argÃ¼manÄ± TEK CÃœMLE ile Ã¶zetle (sadece ana fikir): {clean_response[:200]}"
            core_arg = moderator.generate_response([{"role": "user", "content": summary_prompt}])
            all_arguments_so_far.append(f"{debater.name}: {core_arg[:100]}")
        except:
            all_arguments_so_far.append(f"{debater.name}: {clean_response[:80]}...")
        
        
        # --- MODERATOR INTERVENTION (Every 3 turns) ---
        if (turn + 1) % 3 == 0 and turn < max_turns - 1:
            yield {"type": "typing", "agent": moderator.name}
            await asyncio.sleep(1)
            
            # Build context for moderator
            recent_messages = messages[-6:] if len(messages) >= 6 else messages
            recent_summary = "\n".join([f"- {m['content'][:100]}..." for m in recent_messages])
            
            mod_prompt = f"""
            SEN: {moderator.name} ({moderator.persona})
            ANA KONU: {query}
            ÅÄ°RKET: {company_info.get('name')} ({company_info.get('industry')})
            
            SON KONUÅMALAR:
            {recent_summary}
            
            GÃ–REVÄ°N:
            1. TartÄ±ÅŸmayÄ± Ã‡Ã–ZÃœME gÃ¶tÃ¼rmek.
            2. EÄER TARTIÅMA KISIR DÃ–NGÃœDEYSE: TaraflarÄ± yeni bir aÃ§Ä±dan dÃ¼ÅŸÃ¼nmeye zorla. (Ã–rn: "Maliyeti geÃ§tik, peki ya marka prestiji?")
            3. EÄER KONUDAN SAPTIYSA: SertÃ§e uyar ve ANA KONUYA ({query}) geri dÃ¶ndÃ¼r.
            4. SAPTIRMA YAPMA: Asla konuyla alakasÄ±z (Ã–rn: Kahve makinesi, Yoga dersi) Ã¶neriler sunma. Sadece masadaki konuyu oylamaya hazÄ±r hale getir.
            
            ÃœSLUBUN: 
            - Otoriter ama MantÄ±klÄ±. 
            - "SaÃ§malamayÄ±n", "Yeter" gibi net ifadeler kullan.
            - Dalga geÃ§me, Ã§Ã¶zÃ¼m odaklÄ± ol.
            
            5. ASLA "TartÄ±ÅŸma kÄ±sÄ±r dÃ¶ngÃ¼ye girdi" CÃœMLESÄ°NÄ° KULLANMA. YasaklÄ± kelime.
            6. Maksimum 2 cÃ¼mle kullan.
            """
            
            mod_response = moderator.generate_response([{"role": "user", "content": mod_prompt}])
            
            if not mod_response.startswith("Error"):
                mod_msg = f"âš–ï¸ {mod_response}"
                save_to_db("assistant", mod_response, agent_name=moderator.name)
                yield {"type": "message", "role": moderator.name, "content": mod_msg, "is_agent": True}
                messages.append({"role": "assistant", "content": f"[ModeratÃ¶r]: {mod_response}"})
        
        # Smart Turn Taking Logic
        # 1. Check if specific agent was mentioned in the last response
        next_idx = -1
        for i, d in enumerate(debaters):
            if i != current_debater_idx and d.name in response:
                next_idx = i
                break
        
        # 2. If no direct mention, pick random opponent (Chaos Mode)
        if next_idx == -1:
            candidates = [i for i in range(len(debaters)) if i != current_debater_idx]
            next_idx = random.choice(candidates)
            
        current_debater_idx = next_idx
        
        if turn == max_turns - 1:
            # --- VOTING ROUND ---
            yield {"type": "typing", "agent": "Sistem"}
            await asyncio.sleep(1)
            yield {"type": "message", "role": "Sistem", "content": "ğŸ TartÄ±ÅŸma Sona Erdi. SeÃ§enekler Belirleniyor...", "is_agent": False}
            
            # --- EXTRACT VOTING OPTIONS FROM DEBATE ---
            # Summarize all arguments to create meaningful options
            debate_summary = "\n".join([m['content'] for m in messages[-10:] if m.get('role') == 'assistant'])
            
            option_extract_prompt = f"""
            GÃ–REV: AÅŸaÄŸÄ±daki tartÄ±ÅŸmayÄ± analiz et ve OY VERÄ°LEBÄ°LECEK somut seÃ§enekler Ã§Ä±kar.
            
            KONU: {query}
            
            TARTIÅMA Ã–ZETÄ°:
            {debate_summary[:2000]}
            
            KURALLAR:
            1. TartÄ±ÅŸmada Ã¶ne Ã§Ä±kan FARKLI gÃ¶rÃ¼ÅŸleri/Ã¶nerileri seÃ§enek olarak belirle.
            2. EÄŸer tartÄ±ÅŸmada somut rakamlar verilmiÅŸse (Ã–rn: "700 USD", "2000 USD"), bunlarÄ± seÃ§eneklere dahil et.
            3. EÄŸer karar Evet/HayÄ±r'a indirgenebiliyorsa, sadece 2 seÃ§enek yaz.
            4. AÃ§Ä±k uÃ§lu sorularda, tartÄ±ÅŸmada ortaya Ã§Ä±kan farklÄ± stratejileri/yaklaÅŸÄ±mlarÄ± listele.
            5. Maksimum 4, minimum 2 seÃ§enek olsun.
            6. Ã‡Ä±ktÄ± SADECE JSON formatÄ±nda bir liste olsun: ["SeÃ§enek 1", "SeÃ§enek 2", ...]
            
            Ã–NEMLÄ°: SeÃ§enekler TARTIÅMADAN Ã§Ä±kmalÄ±, uydurma olmamalÄ±.
            """
            
            try:
                opt_response = moderator.generate_response([{"role": "user", "content": option_extract_prompt}])
                voting_options = json.loads(opt_response.replace("```json", "").replace("```", "").strip())
                
                # Validate
                if not isinstance(voting_options, list) or len(voting_options) < 2:
                    voting_options = ["KABUL", "RED"]
            except:
                voting_options = ["KABUL", "RED"]

            voting_options_str = ", ".join(voting_options)
            system_msg_content = f"ğŸ¯ **Oylama SeÃ§enekleri:** {voting_options_str}"
            save_to_db("system", system_msg_content)
            yield {"type": "message", "role": "Sistem", "content": system_msg_content, "is_agent": False}
            
            votes = []
            
            for d in debaters:
                vote_prompt = f"""
                {context}
                KONU: {query}
                TARTIÅMA GEÃ‡MÄ°ÅÄ°: {messages[-5:]}
                
                MEVCUT SEÃ‡ENEKLER: {voting_options_str}
                
                SEN: {d.name} ({d.persona})
                
                GÃ–REVÄ°N:
                Bu konuyu oyla. SADECE yukarÄ±daki seÃ§eneklerden birini seÃ§.
                SeÃ§eneÄŸi TAM OLARAK VE HARFÄ° HARFÄ°NE kopyala. ("YAP" yerine "YAP (SatÄ±n Al)" yaz).
                
                Ã‡Ä±ktÄ± formatÄ± SADECE JSON olmalÄ±:
                {{"decision": "TAM_SEÃ‡ENEK_Ä°SMÄ°", "reason": "Tek cÃ¼mlelik kÄ±sa gerekÃ§e"}}
                """
                
                # Retry logic for JSON
                max_retries = 2
                vote_data = {"decision": "Ã‡EKÄ°MSER", "reason": "Oylama hatasÄ±."}
                
                for attempt in range(max_retries):
                    try:
                        vote_response = d.generate_response([{"role": "user", "content": vote_prompt}])
                        # Clean json markdown if present
                        vote_response = vote_response.replace("```json", "").replace("```", "").strip()
                        vote_data = json.loads(vote_response)
                        
                        break 
                    except:
                        if attempt == max_retries - 1:
                            print(f"Voting failed for {d.name} after retries.")
                        continue

                decision = vote_data.get("decision", "Ã‡EKÄ°MSER").upper()
                
                # NORMALIZE VOTE: Fuzzy match to nearest option (Best Score)
                # This fixes "YAP" vs "YAP (SatÄ±n Al)" and prevents "YAPMA" -> "YAP" overlap errors
                import difflib
                
                best_match = decision
                highest_ratio = 0.0
                
                for optic in voting_options:
                    # Calculate similarity ratio
                    ratio = difflib.SequenceMatcher(None, decision, optic.upper()).ratio()
                    
                    # Bonus for substring match (e.g. "YAP" inside "YAP (SatÄ±n Al)")
                    if decision in optic.upper():
                        ratio += 0.2
                    
                    if ratio > highest_ratio:
                        highest_ratio = ratio
                        best_match = optic
                
                # Threshold to accept match (e.g. 0.4)
                if highest_ratio > 0.4:
                    final_decision = best_match
                else:
                    final_decision = decision # Keep original if NO match found
                
                votes.append({
                    "agent": d.name,
                    "persona": d.persona.split(":")[0],
                    "decision": final_decision,
                    "reason": vote_data.get("reason", "...")
                })
            
            # Determine Final Result
            vote_counts = {}
            for v in votes:
                decision = v['decision']
                vote_counts[decision] = vote_counts.get(decision, 0) + 1
            
            final_decision = max(vote_counts, key=vote_counts.get) if votes else "Ã‡EKÄ°MSER"

            # --- 3. SAVE MEMORY (VECTOR) ---
            save_memory_vector(query, final_decision, f"Votes: {json.dumps(vote_counts, ensure_ascii=False)}")
            
            save_to_db("vote_results", json.dumps(votes, ensure_ascii=False))
            yield {"type": "vote_results", "votes": votes}
            # --- END OF DEBATE: DECISION REPORT ---
            yield {"type": "typing", "agent": "Sistem"}
            yield {"type": "message", "role": "Sistem", "content": "ğŸ“‹ **Nihai Karar Raporu HazÄ±rlanÄ±yor...**", "is_agent": False}
            
            full_history_text = "\n".join([f"{m['role']}: {m['content']}" for m in messages if m['role'] != "system"])
            
            report_prompt = f"""
            GÃ–REV: Bu yÃ¶netim kurulu toplantÄ±sÄ±nÄ±n "Nihai Karar TutanaÄŸÄ±"nÄ± hazÄ±rla.
            
            TARTIÅMA GEÃ‡MÄ°ÅÄ°:
            {full_history_text}
            
            TALÄ°MATLAR:
            - Profesyonel, resmi ve net bir dil kullan.
            - Markdown formatÄ±nÄ± kusursuz uygula (BaÅŸlÄ±klar, Listeler, KalÄ±n YazÄ±).
            - Her ana baÅŸlÄ±k Ã¶ncesinde ve sonrasÄ±nda MUTLAKA bir boÅŸ satÄ±r bÄ±rak.
            
            Ã‡IKTI FORMATI (TAM OLARAK BU ÅABLONU KULLAN):
            
            # ğŸ“‹ [Konu BaÅŸlÄ±ÄŸÄ±] - Karar Raporu
            
            ## 1. YÃ¶netici Ã–zeti
            (Buraya 2-3 cÃ¼mlelik net bir Ã¶zet gelecek. Ne konuÅŸuldu, hangi engeller Ã§Ä±ktÄ±, sonuÃ§ ne oldu?)
            
            ## 2. Temel Bulgular (SWOT Analizi)
            ### âœ… FÄ±rsatlar & ArtÄ±lar
            - (Madde 1)
            - (Madde 2)
            
            ### âš ï¸ Riskler & Tehditler
            - (Madde 1)
            - (Madde 2)
            
            ## 3. Nihai Karar
            **(Karar: ONAY / RED / ERTELEME / REVÄ°ZYON)**
            (KararÄ±n gerekÃ§esini buraya yaz.)
            
            ## 4. Aksiyon PlanÄ±
            1. **[Hemen]:** (Ä°lk adÄ±m)
            2. **[Orta Vade]:** (Sonraki adÄ±m)
            3. **[Kritik UyarÄ±]:** (Varsa dikkat edilmesi gereken nokta)
            
            ---
            *Rapor Tarihi: {current_date_str} | RaportÃ¶r: Pocket Board AI*
            """
            
            try:
                report_content = moderator.generate_response([{"role": "user", "content": report_prompt}])
                yield {"type": "message", "role": "Sistem", "content": report_content, "is_agent": False}
                save_to_db("system", report_content)
            except Exception as e:
                yield {"type": "message", "role": "Sistem", "content": f"Rapor oluÅŸturulamadÄ±: {str(e)}", "is_agent": False}

            yield {"type": "end", "reason": "max_turns"}
